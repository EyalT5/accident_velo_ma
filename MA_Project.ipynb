{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger le dataset\n",
    "file_path = \"accidentsVelo.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Suppression des accidents de gravité inférieure à 3\n",
    "df_cleaned = df[df[\"grav\"] >= 3]\n",
    "\n",
    "# Colonnes ayant trop de valeurs manquantes (plus de 10%)\n",
    "cols_to_drop = [\"lartpc\", \"typevehicules\", \"manoeuvehicules\", \"numVehicules\", \"larrout\"]\n",
    "df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "\n",
    "# Conversion des colonnes mal formatées en float\n",
    "cols_to_convert = [\"long\", \"lat\", \"age\"]\n",
    "for col in cols_to_convert:\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "# Remplissage des valeurs manquantes\n",
    "fill_median = [\"long\", \"lat\", \"age\"]\n",
    "fill_mode = [\"nbv\", \"circ\", \"atm\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\", \"equipement\", \"obs\", \"obsm\", \"choc\", \"manv\", \"col\", \"trajet\"]\n",
    "for col in fill_median:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "for col in fill_mode:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
    "\n",
    "# Extraction de l'heure pour l'analyse horaire\n",
    "df_cleaned[\"heure\"] = df_cleaned[\"hrmn\"].astype(str).str.split(\":\").str[0].astype(int)\n",
    "\n",
    "# Vérification finale des valeurs manquantes\n",
    "df_cleaned.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Distribution des accidents par gravité\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=df_cleaned[\"grav\"], palette=\"coolwarm\")\n",
    "plt.title(\"Distribution des Accidents par Gravité\")\n",
    "plt.xlabel(\"Gravité de l'Accident\")\n",
    "plt.ylabel(\"Nombre d'Accidents\")\n",
    "plt.show()\n",
    "\n",
    "# Répartition des accidents par heure\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_cleaned[\"heure\"], palette=\"viridis\")\n",
    "plt.title(\"Répartition des Accidents par Heure\")\n",
    "plt.xlabel(\"Heure de la Journée\")\n",
    "plt.ylabel(\"Nombre d'Accidents\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Carte statique des accidents\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_cleaned[\"long\"], df_cleaned[\"lat\"], alpha=0.3, s=1, color=\"red\")\n",
    "plt.title(\"Répartition géographique des accidents\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correction des valeurs contenant des \"/\"\n",
    "for col in [\"equipement\", \"choc\", \"manv\"]:\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.split(\"/\").str[0]\n",
    "    df_cleaned[col] = df_cleaned[col].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sélection des features et de la target\n",
    "features = [\"heure\", \"long\", \"lat\", \"nbv\", \"circ\", \"atm\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\", \"equipement\", \"choc\", \"manv\"]\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned[\"grav\"]  # Gravité de l'accident comme variable cible\n",
    "\n",
    "# Correction des valeurs contenant des \"/\"\n",
    "for col in [\"equipement\", \"choc\", \"manv\"]:\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.split(\"/\").str[0]\n",
    "    df_cleaned[col] = df_cleaned[col].astype(float)\n",
    "\n",
    "# Séparation en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialisation des modèles\n",
    "models = {\n",
    "    \"Régression Logistique\": LogisticRegression(max_iter=500),\n",
    "    \"k-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation des modèles\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "    print(f\"Modèle : {name} - Accuracy : {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=list(results.keys()), y=list(results.values()), palette=\"magma\")\n",
    "plt.title(\"Performance des modèles (Accuracy)\")\n",
    "plt.xlabel(\"Modèle\")\n",
    "plt.ylabel(\"Score d'Accuracy\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection du meilleur modèle\n",
    "best_model = models[\"Random Forest\"]\n",
    "df_cleaned[\"grav_pred\"] = best_model.predict(scaler.transform(df_cleaned[features]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install folium\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# Création de la carte interactive\n",
    "m = folium.Map(location=[df_cleaned[\"lat\"].mean(), df_cleaned[\"long\"].mean()], zoom_start=6)\n",
    "\n",
    "# Ajout des points sur la carte avec couleur selon la gravité prédite\n",
    "for _, row in df_cleaned.iterrows():\n",
    "    color = \"red\" if row[\"grav_pred\"] == 4 else \"orange\"\n",
    "    folium.CircleMarker(\n",
    "        location=[row[\"lat\"], row[\"long\"]],\n",
    "        radius=2,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        fill_opacity=0.6,\n",
    "    ).add_to(m)\n",
    "\n",
    "# Ajout d'une Heatmap pour voir les zones critiques\n",
    "heat_data = df_cleaned[[\"lat\", \"long\"]].values.tolist()\n",
    "HeatMap(heat_data, radius=10).add_to(m)\n",
    "\n",
    "# Affichage de la carte interactive\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Création d'un boxplot pour visualiser la distribution de la gravité en fonction de l'heure\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=df_cleaned[\"heure\"], y=df_cleaned[\"grav\"], palette=\"coolwarm\")\n",
    "plt.title(\"Impact de l'heure sur la Gravité des Accidents\")\n",
    "plt.xlabel(\"Heure de la Journée\")\n",
    "plt.ylabel(\"Gravité de l'Accident\")\n",
    "plt.show()\n",
    "# Affichage des fréquences d'accidents pour chaque heure\n",
    "hour_counts = df_cleaned['heure'].value_counts().sort_index()\n",
    "print(hour_counts)\n",
    "\n",
    "# Visualisation des horaires avec un barplot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=hour_counts.index, y=hour_counts.values, palette=\"viridis\")\n",
    "plt.title(\"Nombre d'Accidents par Heure\")\n",
    "plt.xlabel(\"Heure de la Journée\")\n",
    "plt.ylabel(\"Nombre d'Accidents\")\n",
    "plt.xticks(rotation=45)  # Rotation des étiquettes pour une meilleure lisibilité\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regrouper les accidents par département et calculer la gravité moyenne\n",
    "top_departments = df_cleaned.groupby(\"dep\")[\"grav\"].mean().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_departments.index, y=top_departments.values, palette=\"Reds\")\n",
    "plt.title(\"Départements avec les Accidents les Plus Graves\")\n",
    "plt.xlabel(\"Département\")\n",
    "plt.ylabel(\"Gravité Moyenne\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un boxplot pour voir la répartition de l'âge selon la gravité\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=df_cleaned[\"grav\"], y=df_cleaned[\"age\"], palette=\"viridis\")\n",
    "plt.title(\"Âge des Conducteurs en Fonction de la Gravité de l'Accident\")\n",
    "plt.xlabel(\"Gravité de l'Accident\")\n",
    "plt.ylabel(\"Âge du Conducteur\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la matrice de corrélation\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df_cleaned[[\"grav\", \"heure\", \"long\", \"lat\", \"age\", \"nbv\", \"circ\", \"atm\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\", \"equipement\", \"choc\", \"manv\"]].corr()\n",
    "\n",
    "# Affichage sous forme de heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Matrice de Corrélation des Variables\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un countplot pour voir l'influence des conditions météo sur la gravité\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_cleaned[\"atm\"], hue=df_cleaned[\"grav\"], palette=\"coolwarm\")\n",
    "plt.title(\"Impact des Conditions Atmosphériques sur la Gravité des Accidents\")\n",
    "plt.xlabel(\"Conditions Atmosphériques\")\n",
    "plt.ylabel(\"Nombre d'Accidents\")\n",
    "plt.legend(title=\"Gravité\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un countplot pour voir l'influence du type de choc sur la gravité\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=df_cleaned[\"choc\"], hue=df_cleaned[\"grav\"], palette=\"magma\")\n",
    "plt.title(\"Impact du Type de Choc sur la Gravité des Accidents\")\n",
    "plt.xlabel(\"Type de Choc\")\n",
    "plt.ylabel(\"Nombre d'Accidents\")\n",
    "plt.legend(title=\"Gravité\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import joblib\n",
    "\n",
    "# Chargement du dataset\n",
    "file_path = \"accidentsVelo.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Suppression des accidents de gravité inférieure à 3\n",
    "df_cleaned = df[df[\"grav\"] >= 3]\n",
    "\n",
    "# Suppression des colonnes avec trop de valeurs manquantes\n",
    "cols_to_drop = [\"lartpc\", \"typevehicules\", \"manoeuvehicules\", \"numVehicules\", \"larrout\"]\n",
    "df_cleaned = df_cleaned.drop(columns=cols_to_drop)\n",
    "\n",
    "# Conversion des colonnes en float\n",
    "cols_to_convert = [\"long\", \"lat\", \"age\"]\n",
    "for col in cols_to_convert:\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.replace(\",\", \".\").astype(float)\n",
    "\n",
    "# Remplissage des valeurs manquantes\n",
    "fill_median = [\"long\", \"lat\", \"age\"]\n",
    "fill_mode = [\"nbv\", \"circ\", \"atm\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\", \"equipement\", \"obs\", \"obsm\", \"choc\", \"manv\", \"col\", \"trajet\"]\n",
    "for col in fill_median:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)\n",
    "for col in fill_mode:\n",
    "    df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)\n",
    "\n",
    "# Extraction de l'heure de l'accident\n",
    "df_cleaned[\"heure\"] = df_cleaned[\"hrmn\"].astype(str).str.split(\":\").str[0].astype(int)\n",
    "\n",
    "# Correction des valeurs contenant \"/\"\n",
    "for col in [\"equipement\", \"choc\", \"manv\"]:\n",
    "    df_cleaned[col] = df_cleaned[col].astype(str).str.split(\"/\").str[0].astype(float)\n",
    "\n",
    "# Sélection des features et variable cible\n",
    "features = [\"heure\", \"long\", \"lat\", \"nbv\", \"circ\", \"atm\", \"prof\", \"plan\", \"surf\", \"infra\", \"situ\", \"equipement\", \"choc\", \"manv\"]\n",
    "X = df_cleaned[features]\n",
    "y = df_cleaned[\"grav\"]\n",
    "\n",
    "# Séparation en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalisation des features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Grilles de recherche pour l'optimisation\n",
    "param_grid_rf = {\"n_estimators\": [50, 100], \"max_depth\": [None, 10]}\n",
    "param_grid_knn = {\"n_neighbors\": [3, 5]}\n",
    "param_grid_lr = {\"C\": [0.1, 1]}\n",
    "\n",
    "models_params = {\n",
    "    \"Random Forest\": (RandomForestClassifier(random_state=42), param_grid_rf),\n",
    "    \"k-NN\": (KNeighborsClassifier(), param_grid_knn),\n",
    "    \"Régression Logistique\": (LogisticRegression(max_iter=500), param_grid_lr)\n",
    "}\n",
    "\n",
    "# Optimisation et évaluation des modèles\n",
    "best_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for name, (model, params) in models_params.items():\n",
    "    grid_search = GridSearchCV(model, params, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "    \n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_scores[name] = accuracy\n",
    "    \n",
    "    print(f\"Meilleur modèle {name} - Accuracy : {accuracy:.4f}\")\n",
    "    print(f\"Meilleurs paramètres : {grid_search.best_params_}\")\n",
    "    \n",
    "    # Matrice de confusion\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Prédictions\")\n",
    "    plt.ylabel(\"Vraies valeurs\")\n",
    "    plt.title(f\"Matrice de Confusion - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Sauvegarde du meilleur modèle\n",
    "best_model_name = max(model_scores, key=model_scores.get)\n",
    "joblib.dump(best_models[best_model_name], \"meilleur_modele.pkl\")\n",
    "print(f\"Le modèle sauvegardé est : {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
